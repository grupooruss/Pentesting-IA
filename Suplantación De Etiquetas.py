from art.attacks.poisoning import PoisoningAttackBackdoor
from art.attacks.poisoning.perturbations import add_pattern_bd
from art.estimators.classification import SklearnClassifier
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn import datasets
import numpy as np

# Carga un conjunto de datos de ejemplo
iris = datasets.load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

# Crea y entrena un modelo de clasificaci칩n b치sico
model = svm.SVC()
model.fit(X_train, y_train)

# Crea un clasificador ART a partir del modelo de sklearn
art_classifier = SklearnClassifier(model=model)

# Crea un ataque de suplantaci칩n de etiquetas con ART
attack = PoisoningAttackBackdoor(perturbation=add_pattern_bd, poison_fraction=0.1)

# Genera ejemplos envenenados
X_train_adv, y_train_adv = attack.poison(x=X_train, y=y_train)

# Vuelve a entrenar el modelo con los datos envenenados
model.fit(X_train_adv, y_train_adv)

# Verifica la precisi칩n del modelo en el conjunto de prueba original
predictions = model.predict(X_test)
accuracy = np.sum(predictions == y_test) / len(y_test)
print(f'Accuracy on original test examples after poisoning: {accuracy * 100}%')
